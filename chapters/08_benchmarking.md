<!-- EDIT THIS PART VIA 08_benchmarking.md -->

<a name="08-benchmarking"></a>

## Бенчмаркінг вашої моделі

<!-- START_SKIP_FOR_README -->

![Cover Image Benchmarking](https://raw.githubusercontent.com/d-krupke/cpsat-primer/main/images/logo_5.webp)

<!-- STOP_SKIP_FOR_README -->

У цьому розділі розглядаються методи порівняння продуктивності різних моделей,
які застосовуються до складних задач, де базова модель залишає простір для
покращення — або за часом роботи, або за якістю розв’язку, особливо коли модель
не завжди можна розв’язати оптимально. Для дослідника, який пише статтю про
нову модель (або, що частіше, про новий алгоритм, який внутрішньо використовує
модель), це буде типовим випадком, адже дослідження проблеми, яку вже добре
розв’язують, важко опублікувати. Чи ви просто хочете оцінити, чи новий підхід
кращий за поточний, чи готуєте формальну наукову публікацію — виклики однакові;
у другому випадку процес просто стає більш масштабним і формалізованим (хоча
інколи це може стосуватися і першого випадку, залежно від вашого менеджера).

> [!WARNING]
>
> У деяких випадках основне вузьке місце продуктивності може бути не в CP-SAT, а
> в **Python-коді, що генерує модель**. Тому критично важливо знаходити
> найресурсомісткіші ділянки коду. Профайлер
> [Scalene](https://github.com/plasma-umass/scalene) добре зарекомендував себе
> для виявлення таких проблем. У багатьох ситуаціях достатньо навіть простих
> логів (наприклад,
> `logging.info("Building circuit constraint on graph with %d nodes and %d edges", n, m)`),
> щоб побачити базові проблеми продуктивності. Легко недооцінити розмір або
> вартість побудови допоміжних структур, що може суттєво вплинути на загальний
> час виконання.

Під час дослідницької фази, коли ви перевіряєте різні ідеї, ви, ймовірно,
оберете один-п’ять екземплярів, які можна швидко запускати та порівнювати.
Однак для більшості застосувань цього недостатньо, і ви ризикуєте
переадаптувати модель під ці екземпляри — покращивши результат на них, але
погіршивши на інших. Ви навіть можете обмежити здатність моделі розв’язувати
певні екземпляри.

Класичний приклад — вимкнення певних стратегій пошуку CP-SAT або кроків
попередньої обробки, які не дали користі на вибраних екземплярах. Якщо набір
екземплярів достатньо великий, ризик невеликий; але якщо у вас лише кілька
екземплярів, ви можете прибрати єдину стратегію, потрібну для розв’язання
певного класу задач. Сучасні розв’язувачі мають можливості, що додають невеликий
овергед на простих екземплярах, але дозволяють розв’язувати інакше
нерозв’язні випадки. Такий компроміс виправданий: **не жертвуйте здатністю
розв’язувати складні екземпляри заради мізерного виграшу на простих**. Тому
завжди коректно бенчмаркуйте зміни перед впровадженням у продакшн, навіть якщо
не плануєте наукових публікацій.

Зауважте, що цей розділ зосереджується лише на покращенні продуктивності моделі
в межах її конкретного формулювання; він не охоплює оцінку точності моделі чи її
бізнес-цінності. Коли ви розв’язуєте реальну задачу, де модель є лише
наближенням
[реальності](https://en.wikipedia.org/wiki/All_models_are_wrong), важливо також
працювати над якістю наближення і відстежувати реальну ефективність отриманих
рішень. У деяких випадках простіші формулювання не лише дають кращі результати,
а й легше оптимізуються.

### Теорема no-free-lunch і тайм-аути

**Теорема no-free-lunch** і тайм-аути ускладнюють бенчмаркінг більше, ніж може
здатися. Теорема no-free-lunch стверджує, що не існує алгоритму, який
перевершує всі інші на кожному екземплярі, що особливо справедливо для
NP-складних задач. Відповідно, покращення на одних екземплярах часто
супроводжується погіршеннями на інших. Важливо оцінювати, чи виправдані
виграші порівняно з втратами.

Інша проблема виникає, коли ми вводимо ліміт часу, щоб окремі екземпляри не
працювали нескінченно. Без такого ліміту бенчмарки можуть тривати надто довго.
Однак включення перерваних запусків у дані ускладнює оцінку продуктивності,
адже незрозуміло, чи розв’язувач знайшов би рішення одразу після тайм-ауту, чи
застряг у нескінченному пошуку. Якщо відкинути всі екземпляри, що дали
тайм-аут для певної моделі, оцінка обмежиться простими екземплярами, хоча
складніші часто цікавіші. Навпаки, якщо відкинути всі моделі, що дали тайм-аут
на будь-якому екземплярі, можна не залишити жодного кандидата, адже будь-який
розв’язувач імовірно провалиться хоча б на одному екземплярі у достатньо
великому наборі. Чи ви шукаєте доведено оптимальне рішення, найкраще рішення за
фіксований час або просто будь-яке допустиме рішення — важливо забезпечити
порівняння на наборах даних, які містять невідомі результати.

### Приклад: бенчмарк задачі формування графіка медсестер

Розгляньмо продуктивність CP-SAT, Gurobi та Hexaly на задачі формування
графіка медсестер, щоб проілюструвати додаткову складність вибору відповідного
ліміту часу. Це складна, але поширена задача, де медсестер потрібно призначити
на зміни, задовольняючи різноманітні обмеження. Оскільки CP-SAT, Gurobi та
Hexaly суттєво відрізняються за внутрішніми алгоритмами, порівняння показує
виразні відмінності в продуктивності. Подібні патерни можна спостерігати і
в межах одного розв’язувача на різних екземплярах, хоча зазвичай не так
виразно.

Наступні два графіки ілюструють значення поточного найкращого розв’язку
(incumbent) та найкращої доведеної нижньої межі під час пошуку. Це складні
екземпляри, і лише Gurobi знаходить оптимальний розв’язок.

Важливо, що найкращий розв’язувач змінюється залежно від виділеного часу. Для
цієї задачі Hexaly добре знаходить початкові рішення, але потім часто
застрягає. CP-SAT стартує трохи повільніше, зате демонструє стабільний прогрес.
Gurobi ж починає повільно, але з часом робить суттєві покращення.

То який розв’язувач найкращий для цієї задачі?

|                                                                                              ![NRP Instance 19](https://github.com/d-krupke/cpsat-primer/blob/main/images/nrp_19.png?raw=true)                                                                                               |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Порівняння продуктивності CP-SAT, Gurobi та Hexaly на екземплярі 19 бенчмарку графіків медсестер. Hexaly добре стартує, але його згодом обганяє CP-SAT. Gurobi наприкінці трохи випереджає Hexaly. CP-SAT і Gurobi сходяться майже до тієї самої нижньої межі. |

|                                                                                                                                                                               ![NRP Instance 20](https://github.com/d-krupke/cpsat-primer/blob/main/images/nrp_20.png?raw=true)                                                                                                                                                                               |
| :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Порівняння продуктивності CP-SAT, Gurobi та Hexaly на екземплярі 20 бенчмарку графіків медсестер. Hexaly знову добре стартує, але його випереджає CP-SAT. Gurobi більшу частину часу має слабке incumbent-значення, проте зрештою робить значний ривок і доводить оптимальність. Оптимальний розв’язок помітно кращий за найкращий розв’язок CP-SAT. CP-SAT не може довести значущу нижню межу для цього екземпляра. |

> [!WARNING]
>
> Ці два графіки (і навіть ця конкретна задача) недостатні, щоб зробити
> остаточні висновки про загальну продуктивність розв’язувачів. Втім, вражає,
> що наш улюблений open-source розв’язувач CP-SAT тут настільки добре
> конкурує з комерційними Gurobi та Hexaly.

Якщо потрібні доведено оптимальні розв’язки, Gurobi може бути найкращим
вибором; однак імовірність досягти оптимальності низька для більшості
екземплярів. Якщо очікується зростання розміру екземплярів і потрібні швидкі
рішення, Hexaly може бути кращим, бо він дуже ефективно знаходить хороші
розв’язки на початку. Це підтверджують і попередні результати на більших
екземплярах, де ні CP-SAT, ні Gurobi не знаходять жодного допустимого рішення.
Натомість CP-SAT — хороший компроміс: він стартує повільніше, але зберігає
стабільний прогрес протягом усього пошуку.

> [!TIP]
>
> Поширена метрика збіжності —
> [**primal integral**](https://www.sciencedirect.com/science/article/abs/pii/S0167637713001181),
> яка вимірює площу під кривою значення incumbent-розв’язку в часі. Вона дає
> один скаляр, що підсумовує, наскільки швидко розв’язувач покращує найкраще
> відоме рішення. CP-SAT повідомляє споріднену метрику: інтеграл логарифма
> optimality gap, що також враховує якість межі. Ці метрики дають об’єктивну
> оцінку прогресу в часі, хоча можуть не повністю відображати специфіку задачі
> або суб’єктивні пріоритети.

### Визначення цілей бенчмаркінгу

Перший крок — визначити конкретні вимоги і як найкраще відповідно вимірювати
продуктивність розв’язувача. Неможливо вручну будувати графіки для кожного
екземпляра і виставляти оцінки за суб’єктивними враженнями; такий підхід не
масштабується і позбавлений об’єктивності та відтворюваності. Натомість слід
визначити конкретну метрику, що точно відображає ваші цілі. Одна стратегія —
обережно добрати бенчмарк-екземпляри, які, ймовірно, розв’язуються оптимально,
з очікуванням, що тенденції продуктивності узагальняться на більші екземпляри.
Інша — визначити фіксований ліміт часу, який ви готові чекати на розв’язок, і
виміряти, наскільки добре кожен розв’язувач працює в цих межах. Жоден метод
оцінювання не буде ідеальним, але важливо усвідомлювати потенційні загрози
валідності результатів. Розгляньмо кілька типових сценаріїв.

> :reference:
>
> Емпіричні дослідження алгоритмів історично мали певне напруження в академічній
> спільноті, де теоретичні результати часто вважаються більш престижними або
> фундаментальними. Стаття
> _[Needed: An Empirical Science of Algorithms](https://pubsonline.informs.org/doi/epdf/10.1287/opre.42.2.201)_
> Джона Гукера (1994) дає цінну історичну та філософську перспективу щодо цього.

## Типові сценарії бенчмаркінгу та візуалізаційні техніки

На практиці виникає кілька типових сценаріїв бенчмаркінгу. Щоб обрати
відповідну візуалізацію чи метод оцінювання, важливо спочатку визначити, який
сценарій стосується вашого випадку, і розуміти, які інструменти краще підходять
для інших контекстів. Не варто за замовчуванням обирати найвізуально привабливий
або складний графік; натомість оберіть той, що найкраще відповідає вашим
аналітичним цілям. Пам’ятайте, що основне призначення графіка — зробити
табличні дані доступнішими та легшими для інтерпретації. Він не замінює
первинні таблиці і не дає остаточних відповідей.

1. **Екземпляри завжди розв’язуються оптимально, і важливий лише час.** Це
   найпростіший сценарій бенчмаркінгу. Якщо кожен екземпляр розв’язується
   оптимально, а вас цікавить лише час, можна узагальнити продуктивність через
   середній (відносний) час або використати простий box plot. Головна складність
   — вибір відповідного типу середнього (арифметичного, геометричного,
   гармонічного) та добір репрезентативних екземплярів, що відображають умови
   продакшну. У цьому випадку решту розділу можна пропустити.

2. **Шукаються оптимальні або допустимі розв’язки, але їх не завжди вдається
   знайти в межах ліміту часу.** Коли трапляються тайм-аути, часи для
   нерозв’язаних екземплярів стають невідомими, тож традиційні середні значення
   стають ненадійними. У таких випадках **cactus plots** є ефективним способом
   візуалізувати продуктивність розв’язувача навіть за неповних даних.

3. **Мета — знайти найкращий можливий розв’язок за фіксований ліміт часу.**
   Тут фокус на **якості розв’язку за часових обмежень**, а не на тому, чи
   досягнуто оптимальності. **Performance plots** особливо підходять для цього,
   адже показують, наскільки близько кожен розв’язувач чи модель підходить до
   найкращого відомого розв’язку на бенчмарковому наборі.

4. **Аналіз масштабованості: як продуктивність змінюється з розміром екземпляра.**
   Якщо ви оцінюєте, як модель масштабується, тобто який максимальний розмір
   екземпляра вона здатна розв’язати оптимально і як далі росте optimality gap,
   **split plots** — хороший вибір. Вони показують час для розв’язаних
   екземплярів і optimality gap для тих, що перевищили ліміт часу, даючи цілісне
   уявлення про масштабованість.

5. **Порівняння продуктивності за кількома метриками з базовою моделлю.** Коли
   потрібен швидкий, інтуїтивний огляд того, як модель працює за кількома
   метриками (час, значення цілі, нижня межа), ідеально підходять **scatter plots
   with performance zones**. Вони дають наочну порівняльну візуалізацію, що
   допомагає легко помічати викиди та компроміси між вимірами.

> [!TIP]
>
> Використовуйте
> [SIGPLAN Empirical Evaluation Checklist](https://raw.githubusercontent.com/SIGPLAN/empirical-evaluation/master/checklist/checklist.pdf),
> якщо ваша оцінка має відповідати академічним стандартам.

### Швидке порівняння з базовою моделлю за допомогою scatter plot

Діаграми розсіювання із зонами продуктивності, з мого досвіду, дуже ефективні для
швидкого порівняння прототипу з базовою моделлю за кількома метриками. Хоч ці
графіки не дають формальної кількісної оцінки, вони забезпечують чіткий візуальний
огляд того, як змінилася продуктивність. Їхні ключові переваги — інтуїтивна
читабельність та здатність працювати з `NaN`-значеннями. Вони особливо корисні
для виявлення викидів, хоча можуть бути менш ефективними, коли забагато точок
перекривається або діапазони даних сильно різняться (інколи допомагає лог-шкала).

Розгляньмо такий приклад таблиці, що порівнює базову оптимізаційну модель із
прототипом за часом, значенням цілі та нижньою межею. Час обмежений 90 секундами,
і якщо оптимальний розв’язок не знайдено за цей час, значення цілі встановлюється
в `NaN`. Запуск завершується раніше ліміту лише тоді, коли знайдено оптимальний
розв’язок.

<details><summary>Приклад даних</summary>

| instance_name | strategy  | runtime |   objective | lower_bound |
| :------------ | :-------- | ------: | ----------: | ----------: |
| att48         | Prototype | 89.8327 |       33522 |       33522 |
| att48         | Baseline  | 90.1308 |       33522 |       33369 |
| eil101        | Prototype | 90.0948 |         629 |         629 |
| eil101        | Baseline  | 43.8567 |         629 |         629 |
| eil51         | Prototype | 84.8225 |         426 |         426 |
| eil51         | Baseline  | 3.05334 |         426 |         426 |
| eil76         | Prototype | 90.2696 |         538 |         538 |
| eil76         | Baseline  | 4.09839 |         538 |         538 |
| gil262        | Prototype | 90.3314 |       13817 |        2368 |
| gil262        | Baseline  | 90.8782 |        3141 |        2240 |
| kroA100       | Prototype | 90.5127 |       21282 |       21282 |
| kroA100       | Baseline  | 90.0241 |       22037 |       20269 |
| kroA150       | Prototype |  90.531 |       27249 |       26420 |
| kroA150       | Baseline  | 90.3025 |       27777 |       24958 |
| kroA200       | Prototype | 90.0019 |      176678 |       29205 |
| kroA200       | Baseline  | 90.7658 |       32749 |       27467 |
| kroB100       | Prototype | 90.1334 |       22141 |       22141 |
| kroB100       | Baseline  | 90.5845 |       22729 |       21520 |
| kroB150       | Prototype | 90.7107 |      128751 |       26016 |
| kroB150       | Baseline  | 90.9659 |       26891 |       25142 |
| kroB200       | Prototype | 90.7931 |      183078 |       29334 |
| kroB200       | Baseline  | 90.3594 |       34481 |       27708 |
| kroC100       | Prototype | 90.5131 |       20749 |       20749 |
| kroC100       | Baseline  | 90.3035 |       21118 |       20125 |
| kroD100       | Prototype | 90.0728 |       21294 |       21294 |
| kroD100       | Baseline  | 90.2563 |       21294 |       20267 |
| kroE100       | Prototype | 90.4515 |       22068 |       22053 |
| kroE100       | Baseline  | 90.6112 |       22341 |       21626 |
| lin105        | Prototype | 90.4714 |       14379 |       14379 |
| lin105        | Baseline  | 90.6532 |       14379 |       13955 |
| lin318        | Prototype | 90.8489 |      282458 |       41384 |
| lin318        | Baseline  | 90.5955 |      103190 |       39016 |
| linhp318      | Prototype | 90.9566 |         nan |       41412 |
| linhp318      | Baseline  | 90.7038 |       84918 |       39016 |
| pr107         | Prototype | 90.3708 |       44303 |       44303 |
| pr107         | Baseline  | 90.4465 |       45114 |       27784 |
| pr124         | Prototype | 90.1689 |       59167 |       58879 |
| pr124         | Baseline  | 90.8673 |       60760 |       52392 |
| pr136         | Prototype | 90.0296 |       96781 |       96772 |
| pr136         | Baseline  | 90.2636 |       98850 |       89369 |
| pr144         | Prototype | 90.3141 |       58537 |       58492 |
| pr144         | Baseline  | 90.6465 |       59167 |       33809 |
| pr152         | Prototype | 90.4742 |       73682 |       73682 |
| pr152         | Baseline  | 90.8629 |       79325 |       46604 |
| pr226         | Prototype | 90.1845 | 1.19724e+06 |       74474 |
| pr226         | Baseline  | 90.6676 |      103271 |       55998 |
| pr264         | Prototype | 90.4012 |      736226 |       41020 |
| pr264         | Baseline  | 90.9642 |       68802 |       37175 |
| pr299         | Prototype | 90.3325 |         nan |       47375 |
| pr299         | Baseline  |   90.19 |      120489 |       45594 |
| pr439         | Prototype | 90.5761 |         nan |       95411 |
| pr439         | Baseline  |  90.459 |      834126 |       93868 |
| pr76          | Prototype | 90.2718 |      108159 |      107727 |
| pr76          | Baseline  | 90.2951 |      110331 |      105340 |
| st70          | Prototype | 90.2824 |         675 |         675 |
| st70          | Baseline  | 90.1484 |         675 |         663 |

</details>

З таблиці вже видно деякі фундаментальні проблеми. Наприклад, прототип
провалюється на трьох екземплярах, а на кількох інших дає суттєво гірші
результати, ніж базова модель. Проте на scatter plot із зонами продуктивності
такі аномалії стають одразу помітними.

|                                                                                                                                              ![Scatter Plot with Performance Zones](https://raw.githubusercontent.com/d-krupke/cpsat-primer/main/images/scatter_tsp.png)                                                                                                                                              |
| :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Scatter plot, що порівнює продуктивність прототипу з базовою моделлю за трьома метриками: час, значення цілі та нижня межа. Вісь x відображає продуктивність базової моделі, вісь y — продуктивність прототипу. Кольорові зони показують відносні рівні продуктивності, що полегшує ідентифікацію ділянок, де прототип кращий або гірший за базову модель. |

За часом обидві моделі зазвичай упираються в ліміт, тож варіативність невелика.
Однак базова модель розв’язує кілька екземплярів значно швидше, тоді як
прототип стабільно використовує весь ліміт. За значенням цілі обидві моделі
переважно дають подібні результати. Проте, особливо на великих екземплярах,
прототип дає або дуже погані, або взагалі не знаходить розв’язків.

Цікаво, що нижні межі прототипу для деяких екземплярів значно кращі. Це не було
очевидним при поверхневому перегляді таблиці, але стає помітним на графіках.

Діаграми розсіювання також дуже ефективні, коли працюєте з кількома метриками
продуктивності, особливо якщо потрібно переконатися, що виграш в одній метриці
не супроводжується неприйнятними втратами в іншій. На практиці часто складно
заздалегідь точно визначити відносну важливість кожної метрики. Інтуїтивність
цих графіків дає цінний огляд і слугує візуальною підказкою перед тим, як ви
зафіксуєте конкретну метрику продуктивності. Нижче показано гіпотетичний приклад
для задачі маршрутизації транспортних засобів.

|                                                                                                                                                             ![Scatter Plot for Multi-Objectives](https://raw.githubusercontent.com/d-krupke/cpsat-primer/main/images/scatter_performance_zones.png)                                                                                                                                                              |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Scatter plots, що ілюструють компроміси продуктивності за кількома метриками в гіпотетичній задачі маршрутизації. Вони допомагають оцінити, чи покращення однієї метрики не супроводжується значними регресіями в іншій. Інтуїтивне розташування робить їх особливо корисними, коли пріоритети метрик ще не визначені, даючи швидкий огляд відносної продуктивності та підсвічуючи викиди між версіями алгоритмів. |

<details>
<summary>Ось код, який я використав для побудови графіків. Його можна вільно копіювати та використовувати.</summary>

```python
"""
This module contains functions to plot a scatter comparison of baseline and new values with performance areas highlighted.

You can freely use and distribute this code under the MIT license.

Changelog:
    2024-08-27: First version
    2024-08-29: Added lines to the diagonal to help with reading the plot
    2025-06-07: Basic improvements and fixing issue with index comparison.

(c) 2025 Dominik Krupke, https://github.com/d-krupke/cpsat-primer
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


def plot_performance_scatter(
    ax,
    baseline: pd.Series,
    new_values: pd.Series,
    lower_is_better: bool = True,
    title: str = "",
    **kwargs,
):
    """
    Plot a scatter comparison of baseline and new values with performance areas highlighted.

    Parameters:
        ax (matplotlib.axes.Axes): The axes on which to plot.
        baseline (pd.Series): Series of baseline values.
        new_values (pd.Series): Series of new values.
        lower_is_better (bool): If True, lower values indicate better performance.
        title (str): Title of the plot.
        **kwargs: Additional keyword arguments for customization (e.g., 'color', 'marker').
    """
    if not isinstance(baseline, pd.Series) or not isinstance(new_values, pd.Series):
        raise ValueError("Both baseline and new_values should be pandas Series.")
    if baseline.size != new_values.size:
        raise ValueError("Both Series should have the same length.")

    scatter_kwargs = {
        "color": kwargs.get("color", "blue"),
        "marker": kwargs.get("marker", "x"),
        "label": kwargs.get("label", "Data Points"),
    }

    line_kwargs = {
        "color": kwargs.get("line_color", "k"),
        "linestyle": kwargs.get("line_style", "--"),
        "label": kwargs.get("line_label", "No Change"),
    }

    fill_improve_kwargs = {
        "color": kwargs.get("improve_color", "green"),
        "alpha": kwargs.get("improve_alpha", 0.3),
        "label": kwargs.get("improve_label", "Improved Performance"),
    }

    fill_decline_kwargs = {
        "color": kwargs.get("decline_color", "red"),
        "alpha": kwargs.get("decline_alpha", 0.3),
        "label": kwargs.get("decline_label", "Declined Performance"),
    }

    # Replace inf values with NaN
    baseline = baseline.replace([np.inf, -np.inf], np.nan)
    new_values = new_values.replace([np.inf, -np.inf], np.nan)

    max_val = max(baseline.max(skipna=True), new_values.max(skipna=True)) * 1.05
    min_val = min(baseline.min(skipna=True), new_values.min(skipna=True)) * 0.95

    # get indices of NA values
    na_indices = baseline.isna() | new_values.isna()

    if lower_is_better:
        # replace NA values with max_val
        baseline = baseline.fillna(max_val)
        new_values = new_values.fillna(max_val)
    else:
        # replace NA values with min_val
        baseline = baseline.fillna(min_val)
        new_values = new_values.fillna(min_val)

    # plot the na_indices with a different marker
    if na_indices.any():
        ax.scatter(
            baseline[na_indices],
            new_values[na_indices],
            marker="s",
            color=scatter_kwargs["color"],
            label="N/A Values",
            zorder=2,
        )

    # add the rest of the data points
    ax.scatter(
        baseline[~na_indices], new_values[~na_indices], **scatter_kwargs, zorder=2
    )

    ax.plot([min_val, max_val], [min_val, max_val], zorder=1, **line_kwargs)

    x = np.linspace(min_val, max_val, 500)
    if lower_is_better:
        ax.fill_between(x, min_val, x, zorder=0, **fill_improve_kwargs)
        ax.fill_between(x, x, max_val, zorder=0, **fill_decline_kwargs)
    else:
        ax.fill_between(x, x, max_val, zorder=0, **fill_improve_kwargs)
        ax.fill_between(x, min_val, x, zorder=0, **fill_decline_kwargs)

    # draw thin lines to the diagonal to help with reading the plot.
    # A problem without lines is that one tends to use the distance
    # to the diagonal as a measure of performance, which is not correct.
    # Instead, it is `y-x` that should be used.
    for old_val, new_val in zip(baseline, new_values):
        if pd.isna(old_val) and pd.isna(new_val):
            continue
        if pd.isna(old_val):
            old_val = min_val if lower_is_better else max_val
        if pd.isna(new_val):
            new_val = min_val if lower_is_better else max_val
        if lower_is_better and new_val < old_val:
            ax.plot(
                [old_val, old_val],
                [old_val, new_val],
                color="green",
                linewidth=1.0,
                zorder=1,
            )
        elif not lower_is_better and new_val > old_val:
            ax.plot(
                [old_val, old_val],
                [old_val, new_val],
                color="green",
                linewidth=1.0,
                zorder=1,
            )
        elif lower_is_better and new_val > old_val:
            ax.plot(
                [old_val, old_val],
                [old_val, new_val],
                color="red",
                linewidth=1.0,
                zorder=1,
            )
        elif not lower_is_better and new_val < old_val:
            ax.plot(
                [old_val, old_val],
                [old_val, new_val],
                color="red",
                linewidth=1.0,
                zorder=1,
            )

    ax.set_xlim(min_val, max_val)
    ax.set_ylim(min_val, max_val)
    ax.set_xlabel(kwargs.get("xlabel", "Baseline"))
    ax.set_ylabel(kwargs.get("ylabel", "New Values"))
    if title:
        ax.set_title(title)
    ax.legend()


def plot_comparison_grid(
    baseline_data: pd.DataFrame,
    new_data: pd.DataFrame,
    metrics: list[tuple[str, str]],
    n_cols: int = 4,
    figsize: tuple[int, int] | None = None,
    suptitle: str = "",
    subplot_kwargs: dict | None = None,
    **kwargs,
):
    """
    Plot a grid of performance comparisons for multiple metrics.

    Parameters:
        baseline_data (pd.DataFrame): DataFrame containing the baseline data.
        new_data (pd.DataFrame): DataFrame containing the new data.
        metrics (list of tuple of str): List of tuples containing column names and comparison direction ('min' or 'max').
        n_cols (int): Number of columns in the grid.
        figsize (tuple of int): Figure size (width, height).
        suptitle (str): Title for the entire figure.
        **kwargs: Additional keyword arguments to pass to individual plot functions.

    Returns:
        fig (matplotlib.figure.Figure): The figure object containing the plots.
        axes (np.ndarray): Array of axes objects corresponding to the subplots.
    """
    n_metrics = len(metrics)
    n_cols = min(n_cols, n_metrics)
    n_rows = (n_metrics + n_cols - 1) // n_cols  # Ceiling division

    # Validate columns and directions
    for column, direction in metrics:
        if direction not in {"min", "max"}:
            raise ValueError("The direction should be either 'min' or 'max'.")
        if column not in baseline_data.columns or column not in new_data.columns:
            raise ValueError(f"Column '{column}' not found in the data.")

    # Validate index alignment
    if set(baseline_data.index) != set(new_data.index):
        raise ValueError("Indices of the DataFrames do not match (different values).")


    if figsize is None:
        figsize = (5 * n_cols, 5 * n_rows)

    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)
    axes = axes.flatten()

    for ax, (column_name, direction) in zip(axes, metrics):
        # Merge kwargs and subplot_kwargs[column_name] (if present) into a new dict
        merged_kwargs = dict(kwargs)
        if subplot_kwargs and column_name in subplot_kwargs:
            merged_kwargs.update(subplot_kwargs[column_name])
        plot_performance_scatter(
            ax,
            baseline_data[column_name],
            new_data[column_name],
            lower_is_better=(direction == "min"),
            title=column_name,
            **merged_kwargs,
        )

    # Turn off any unused subplots
    for ax in axes[n_metrics:]:
        ax.axis("off")

    if suptitle:
        fig.suptitle(suptitle, fontsize=16)

    plt.tight_layout(rect=[0, 0, 1, 0.96])

    return fig, axes
```

</details>

### Бенчмаркінг за фактом успіху: cactus plots і PAR-метрики

У спільноті SAT часто використовують cactus plots (також відомі як survival
plots), щоб ефективно порівнювати час до успіху різних розв’язувачів на
бенчмарковому наборі, навіть коли трапляються тайм-аути. Якщо ви працюєте з
чистою задачею задоволення обмежень, цей підхід застосовується безпосередньо.
Втім, його можна поширити й на інші бінарні індикатори успіху — наприклад,
доведення оптимальності, навіть із допусками оптимальності.

Додатково метрика **PAR10** широко використовується для підсумовування
продуктивності розв’язувача на бенчмарковому наборі. Вона визначається як
середній час розв’язання екземпляра, де нерозв’язані екземпляри (в межах ліміту
часу) штрафуються шляхом присвоєння часу, що дорівнює 10-кратному cutoff.
Зустрічаються і варіанти на кшталт **PAR2**, де штрафний коефіцієнт дорівнює 2.
Хоча множник 10 — конвенційний, це все одно довільний вибір. Зрештою, вам
потрібно вирішити, як обробляти «невідомі» (екземпляри, що не розв’язані в межах
ліміту), адже ви лише знаєте, що їхній фактичний час перевищує cutoff. Якщо для
визначення переможця потрібна явна метрика продуктивності, PAR-метрики широко
прийняті, але мають помітні обмеження.

Щоб отримати більш нюансоване уявлення про продуктивність, часто використовують
**cactus plots**. На таких графіках кожен розв’язувач представлений лінією, де
кожна точка $(x, y)$ означає, що $y$ бенчмарк-екземплярів розв’язано за $x$
секунд (існує також обернена версія).

| ![Cactus Plot 1](https://github.com/d-krupke/cpsat-primer/blob/main/evaluations/tsp/2023-11-18_random_euclidean/PUBLIC_DATA/cactus_plot.png?raw=true) |
| :---------------------------------------------------------------------------------------------------------------------------------------------------: |
|                                     Кожна точка $(x, y)$ означає, що $x$ екземплярів було розв’язано за $y$ секунд.                                      |

Середні значення PAR10 для чотирьох стратегій у прикладі вище такі:

| Strategy             |       PAR10 |
| :------------------- | ----------: |
| AddCircuit           |  512.133506 |
| Dantzig (Gurobi)     |   66.452202 |
| Iterative Dantzig    |  752.412118 |
| Miller-Tucker-Zemlin | 1150.014846 |

Якщо вам цікаво, це дані щодо розв’язання задачі комівояжера (TSP) різними
стратегіями. Gurobi домінує, і це не дивно — добре відомо, що Gurobi чудово
розв’язує TSP.

Якщо кількість розв’язувачів або моделей у порівнянні не надто велика, можна
використати варіацію cactus plot, щоб показати продуктивність при різних
**допусках оптимальності**. Це дозволяє оцінити, наскільки покращується
продуктивність при послабленні допуску. Втім, якщо основний інтерес —
**якість розв’язку**, performance plots, імовірно, будуть доречнішими.

У наступному прикладі різні допуски оптимальності демонструють помітне
покращення для стратегій `AddCircuit` і `Miller-Tucker-Zemlin`. Для інших двох
стратегій вплив зміни допусків мінімальний. Цю варіацію cactus plot можна також
застосувати для порівняння продуктивності на різних бенчмаркових наборах,
особливо якщо ви підозрюєте значні відмінності між категоріями екземплярів.

| ![Cactus Plot with Optimality Tolerances](https://github.com/d-krupke/cpsat-primer/blob/main/evaluations/tsp/2023-11-18_random_euclidean/PUBLIC_DATA/cactus_plot_opt_tol.png?raw=true) |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|             Кожен стиль лінії відповідає допуску оптимальності. Графік показує, скільки екземплярів ($y$) можна розв’язати в межах заданого ліміту часу ($x$) для кожного допуску.              |

Також поширено додавати на cactus plot лінію **virtual best**. Вона показує
для кожного екземпляра найкращий час, досягнутий будь-яким розв’язувачем. Хоча
це не реальний розв’язувач, така лінія є корисним еталоном для оцінки
комплементарності. Якщо один розв’язувач явно домінує, лінія virtual best
збігатиметься з його кривою. Якщо ж криві розходяться, це означає, що жоден
розв’язувач не є універсально кращим — прояв принципу «no free lunch». Навіть
якщо один розв’язувач найкращий на 90% екземплярів, решту 10% можуть краще
закривати альтернативи. Чим більший розрив між найкращим реальним розв’язувачем
і virtual best, тим сильніший аргумент на користь портфельного підходу.

> :reference:
>
> Детальніше про цей тип графіків можна прочитати в академічній статті:
> [Benchmarking Solvers, SAT-style by Brain, Davenport, and Griggio](http://www.sc-square.org/CSA/workshop2-papers/RP3-FinalVersion.pdf)

### Performance plots (профілі продуктивності) для якості розв’язку за ліміту часу

Коли маєте справу з екземплярами, які зазвичай не вдається розв’язати
оптимально, **performance plots** часто доречніші за cactus plots. Ці графіки
показують відносну продуктивність різних моделей або розв’язувачів на наборі
екземплярів, зазвичай із фіксованим лімітом часу. У лівій крайній точці графіка
(де $x = 1$) кожна лінія розв’язувача показує частку екземплярів, на яких він
досяг найкращого відомого розв’язку (не обов’язково унікально). Далі координати
$(x,y)$ означають частку $y$ екземплярів, для яких розв’язувач знайшов розв’язок,
що не більше ніж у $x$ разів гірший за найкращий відомий. Наприклад, якщо є
точка $(1.05, 0.8)$, це означає, що для 80% екземплярів розв’язувач знайшов
розв’язок не гірший за 5% від найкращого. Часто для осі x використовують
логарифмічну шкалу, особливо коли відношення продуктивності сильно варіюються.
Однак нижче ми використовуємо лінійну шкалу, бо значення близькі до 1.

У прикладі нижче, на основі **задачі маршрутизації транспортних засобів із місткістю (CVRP)**,
performance plots порівнюють три різні моделі на бенчмарковому наборі. Ці
графіки дають наочне уявлення про те, наскільки кожна модель наближається до
найкращого розв’язку.

|                                                                                                                                 ![Performance Plot Objective](https://github.com/d-krupke/cpsat-primer/blob/main/images/performance_plot_objective.png?raw=true)                                                                                                                                  |
| :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Performance plot, що порівнює значення цілі різних CVRP-моделей на бенчмарковому наборі. Модель Miller–Tucker–Zemlin найкраща на більшості екземплярів і залишається близькою до найкращої на решті. Дві інші моделі знаходять найкращий розв’язок лише приблизно у 10% випадків, але розв’язують близько 70% екземплярів у межах 2% від найкращого відомого рішення, де `multiple_circuit` має невелику перевагу. |

Звісно, це можна зробити і для нижніх меж, які дає кожна модель.

|                                                                               ![Performance Plot Lower Bound](https://github.com/d-krupke/cpsat-primer/blob/main/images/performance_plot_bound.png?raw=true)                                                                               |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Performance plot, що порівнює нижні межі для кожної CVRP-моделі. Модель `add_circuit` стабільно дає найкращі межі, тоді як дві інші моделі мають межі до 20% гірші в кращому випадку і до 100% гірші (тобто вдвічі гірша якість) на деяких екземплярах. |

<details>
<summary>Ось код, який я використав для побудови графіків. Його можна вільно копіювати та використовувати.</summary>

```python
# MIT License
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.axes import Axes


def plot_performance_profile(
    data: pd.DataFrame,
    instance_column: str,
    strategy_column: str,
    metric_column: str,
    direction: str,
    comparison: str = "relative",
    title: str | None = None,
    highlight_best: bool = False,
    ax: Axes | None = None,
    scale: str | None = None,
    log_base: int = 2,
    figsize: tuple = (9, 6),
) -> Axes:
    """
    Plot a performance profile, either on a relative-ratio basis or absolute-difference basis:
      - For comparison="relative":
          x-axis: performance ratio τ (log scale if τ_max > 10)
          τ = (value / best) if direction="min", or τ = (best / value) if direction="max".
      - For comparison="absolute":
          x-axis: absolute difference Δ = (value - best) if direction="min",
                                      or Δ = (best - value) if direction="max".
      - y-axis: proportion of problems with τ (or Δ) ≤ x for each solver.
      - If highlight_best=True, detect and bold the dominating solver curve (AUC in appropriate space).
      - Ensures a reasonable number of ticks on the x-axis.

    Args:
        data: DataFrame with columns [instance, strategy, metric].
        instance_column: column name identifying each problem instance.
        strategy_column: column name identifying each solver/strategy.
        metric_column: column name of the performance metric (e.g. runtime or cost).
        direction: "min" if lower metric → better, "max" if higher → better.
        comparison: "relative" or "absolute".
        title: Optional plot title.
        highlight_best: If True, find the solver with largest AUC and draw it in bold.
        ax: An existing matplotlib Axes to draw into. If None, a new Figure+Axes will be created using figsize.
        scale: x-axis scale override ("linear" or "log"); if None, chosen automatically.
        log_base: base for log scale if used (default 2).
        figsize: Tuple (width, height). Only used if ax is None.

    Returns:
        The matplotlib Axes containing the performance profile.
    """
    if direction not in ("min", "max"):
        raise ValueError("`direction` must be 'min' or 'max'.")
    if comparison not in ("relative", "absolute"):
        raise ValueError("`comparison` must be 'relative' or 'absolute'.")

    # 1) Compute best value per instance
    best_val = data.groupby(instance_column)[metric_column].agg(direction)

    # 2) Pivot to get per-instance × per-strategy medians
    pivot = (
        data.groupby([instance_column, strategy_column])[metric_column]
        .median()
        .unstack(fill_value=np.nan)
    )

    # 3) Build comparison matrix C[p, s]
    comp = pd.DataFrame(index=pivot.index, columns=pivot.columns, dtype=float)

    if comparison == "relative":
        for strat in pivot.columns:
            if direction == "min":
                comp[strat] = pivot[strat] / best_val
            else:  # direction == "max"
                comp[strat] = best_val / pivot[strat]
        comp = comp.replace([np.inf, -np.inf, 0.0], np.nan)

    else:  # comparison == "absolute"
        for strat in pivot.columns:
            if direction == "min":
                comp[strat] = pivot[strat] - best_val
            else:  # direction == "max"
                comp[strat] = best_val - pivot[strat]
        comp = comp.replace([np.inf, -np.inf], np.nan)

    # 4) Collect all distinct x-values (τ or Δ), including baseline
    all_vals = comp.values.flatten()
    finite_vals = all_vals[np.isfinite(all_vals)]
    baseline = 1.0 if comparison == "relative" else 0.0
    all_x = np.unique(np.sort(finite_vals))
    all_x = np.concatenate(([baseline], all_x))
    all_x = np.unique(np.sort(all_x))

    # 5) Build performance-profile DataFrame ρ(x)
    n_instances = comp.shape[0]
    profile = pd.DataFrame(index=all_x, columns=comp.columns, dtype=float)

    for x in all_x:
        leq = (comp <= x).sum(axis=0)
        profile.loc[x] = leq / n_instances

    # 6) Identify dominating solver if requested (max AUC)
    best_solver = None
    if highlight_best:
        if comparison == "relative":
            # integrate ρ(τ) w.r.t. log(τ)
            log_x = np.log(all_x)
            areas = {}
            for strat in profile.columns:
                y = profile[strat].astype(float).values
                areas[strat] = np.trapz(y, x=log_x)
            best_solver = max(areas, key=areas.get)
        else:
            # integrate ρ(Δ) w.r.t. Δ
            areas = {}
            for strat in profile.columns:
                y = profile[strat].astype(float).values
                areas[strat] = np.trapz(y, x=all_x)
            best_solver = max(areas, key=areas.get)

    # 7) Create or use existing Axes
    if ax is None:
        fig, ax = plt.subplots(figsize=figsize)
    else:
        fig = ax.figure

    # 8) Determine scale if not overridden
    if scale is None:
        if comparison == "relative" and all_x[-1] > 10:
            use_log = True
        else:
            use_log = False
    else:
        use_log = scale == "log"

    # 9) Plot each solver’s curve
    for strat in profile.columns:
        y = profile[strat].astype(float)
        if highlight_best and strat == best_solver:
            ax.step(all_x, y, where="post", label=strat, linewidth=3.0, alpha=1.0)
        else:
            ax.step(
                all_x,
                y,
                where="post",
                label=strat,
                linewidth=1.5,
                alpha=0.6 if highlight_best else 1.0,
            )

    # 10) Axis scaling and limits
    if comparison == "relative":
        if use_log:
            ax.set_xscale("log", base=log_base)
            ax.set_xlim(all_x[1], all_x[-1] * 1.1)
        else:
            ax.set_xscale("linear")
            ax.set_xlim(1.0, all_x[-1] * 1.1)
        xlabel = (
            f"Within this factor of the best (log{log_base} scale)"
            if use_log
            else "Within this factor of the best (linear scale)"
        )
    else:  # absolute
        ax.set_xscale("linear")
        ax.set_xlim(0.0, all_x[-1] * 1.1)
        xlabel = "Absolute difference from the best"

    ax.set_ylim(0.0, 1.02)
    ax.set_xlabel(xlabel, fontsize=12)
    ax.set_ylabel("Proportion of problems", fontsize=12)

    if title:
        ax.set_title(title, fontsize=14, pad=14)
    else:
        ax.set_title("Performance Profile", fontsize=14, pad=14)

    ax.axvline(x=baseline, color="gray", linestyle="--", alpha=0.7)
    ax.grid(True, which="both", linestyle=":", linewidth=0.5)

    # 11) Legend inside lower right
    ax.legend(loc="lower right", frameon=False)

    fig.tight_layout()
    return ax
```

</details>

> :reference:
>
> Танґі Міґо написав чудову статтю про
> [Performance Plots](https://tmigot.github.io/posts/2024/06/teaching/). Також
> варто глянути на оригінальну роботу
> [Benchmarking optimization software with performance profiles (Dolan & Moré 2002)](https://link.springer.com/article/10.1007/s101070100263)

### Аналіз масштабованості однієї моделі

Якщо ви працюєте з однією моделлю й хочете оцінити її **масштабованість**, то
**split plot** є ефективною візуалізацією. Такий графік показує час роботи
моделі на екземплярах різного розміру за фіксованого ліміту часу. Якщо екземпляр
розв’язано в межах ліміту, показується реальний час. Якщо ні — точка
проектується вище ліміту на **трансформованій осі y**, яка відображає
**optimality gap** замість часу.

Приклад такого графіка наведено нижче. Оскільки агрегація цих даних у лінійний
графік може бути складною, візуалізація може «захаращуватися», якщо включити
надто багато екземплярів або одночасно порівнювати кілька моделей.

|                                                                                                  ![Split Plot](https://raw.githubusercontent.com/d-krupke/cpsat-primer/main/images/split_plot.png)                                                                                                   |
| :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Split plot, що ілюструє час роботи (для розв’язаних екземплярів) і optimality gap (для нерозв’язаних). Вісь y поділено на дві області: одна показує фактичні часи для екземплярів, розв’язаних у межах ліміту, а інша — optimality gap для екземплярів, що перевищили ліміт. |

> [!WARNING]
>
> Для багатьох задач не існує єдиної метрики розміру екземпляра. Зазвичай ви
> все одно можете класифікувати екземпляри за категоріями розміру. Однак для
> особливо складних задач, можливо, найкраще просто подати таблицю з результатами
> для найбільших екземплярів, щоб дати уявлення про масштабованість моделі.

### Важливість таблиць

Таблиці дають стислий і детальний огляд результатів бенчмаркінгу. Вони
дозволяють читачам перевіряти коректність даних, розглядати окремі екземпляри та
доповнювати високорівневі візуальні підсумки, наприклад графіки.

Хоч попередні розділи показали корисні графіки для візуалізації тенденцій
продуктивності, важливо включити принаймні одну таблицю з «сирими» результатами
для ключових бенчмарк-екземплярів. Багато якісних робіт спираються лише на
таблиці, адже вони забезпечують прозорість і точність.

Втім, не варто включати всі таблиці з усіма даними — це стосується навіть
додатків. Натомість подумайте, яка інформація потрібна критичному читачеві, щоб
переконатися, що графіки не вводять в оману. Зосередьтеся на найрелевантніших і
найінтерпретованіших результатах. Повний набір даних можна дати у зовнішньому
репозиторії, але таблиці в статті мають бути чіткими, вибірковими та по суті.
Навіть якщо ви оптимізуєте лише для себе, використовуйте графіки для огляду, але
обов’язково переглядайте таблиці з даними.

|                                                       ![Table with Results](https://raw.githubusercontent.com/d-krupke/cpsat-primer/main/images/table_samplns.png)                                                        |
| :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
| Приклад таблиці з недавньої публікації, що подає детальні результати нового алгоритму на бенчмарк-екземплярах. Хоч такі таблиці менш інтуїтивні за графіки, вони дозволяють детально розглянути окремі результати. |

## Розрізнення exploratory та workhorse досліджень у бенчмаркінгу

Перш ніж переходити до повномасштабного бенчмаркінгу для наукових публікацій,
важливо провести попередні дослідження, щоб оцінити можливості моделі та
виявити базові проблеми. Ця фаза, відома як _exploratory studies_
(дослідницькі), є критичною для створення основи більш детального
бенчмаркінгу, що згодом називається _workhorse studies_ (основні). Останні
мають на меті дати надійні відповіді на конкретні дослідницькі запитання і
часто є ядром академічних публікацій. Важливо чітко розрізняти ці два типи
досліджень і зберігати їхні різні цілі: exploratory — для первинного
розуміння та гнучкості, workhorse — для суворих і відтворюваних досліджень.

> :reference:
>
> Для ґрунтовного огляду бенчмаркінгу рекомендую книгу Кетрін К. МакҐіоч
> ["A Guide to Experimental Algorithmics"](https://www.cambridge.org/core/books/guide-to-experimental-algorithmics/CDB0CB718F6250E0806C909E1D3D1082),
> яка глибоко розглядає цю тему.

### Дослідницькі (exploratory studies): закладання основи

Дослідницькі (exploratory) — це перший крок до розуміння і моделі, і задачі, яку
вона має розв’язувати. Ця фаза спрямована на формування інтуїції та виявлення
ключових характеристик, перш ніж переходити до формального бенчмаркінгу.

- **Мета**: на цьому етапі потрібно отримати ранні інсайти, а не робити
  остаточні висновки. Дослідницькі (exploratory) допомагають оцінити
  реалістичні розміри екземплярів, передбачити потенційні труднощі та звузити
  простір пошуку гіперпараметрів.

У цій фазі не варто будувати складні бенчмаркові фреймворки. Тримайте процес
легким і гнучким, щоб швидко ітерувати. Якщо оновлення бенчмарків стає
незручним щоразу, коли ви змінюєте модель, це сповільнить прогрес і — з огляду
на те, що бенчмарковий код часто нудний — швидко знизить мотивацію.

З власного досвіду: моя продуктивність суттєво впала, коли я вперше навчився
будувати надійні бенчмаркові налаштування. Я почав застосовувати той самий рівень
суворості і в дослідницькій фазі, помилково сприймаючи сетап як одноразову
інвестицію, що окупиться згодом. Але щойно ви робите щось справді цікаве,
неминуче виникають несподівані проблеми, які потребують нових ітерацій над
сетапом. Намагаючись передбачити такі сюрпризи, легко переінженерити процес —
витрачаючи надто багато часу на «що може піти не так» замість того, щоб просто
почати.

Натомість тримайте баланс: не допускайте хаосу, але відкладіть формальний
бенчмаркінг, доки не будете готові ділитися результатами. Наприклад, я робив
швидкі дослідницькі (exploratory) дослідження в одному Jupyter notebook, щоб оцінити
відповідні розміри екземплярів для бенчмарк-графіків, показаних раніше. Це не
було надійною частиною пайплайна, але швидко дало потрібний результат — і лише
потім я зібрав повноцінний пайплайн для фінальних графіків і таблиць.

### Основні (workhorse studies): проведення глибоких оцінювань

Основні (workhorse) ідуть після дослідницької (exploratory) фази й
характеризуються більш структурованими та ретельними методологіями. Цей етап
критично важливий для всебічної оцінки моделі та збору суттєвих даних для
аналізу.

- **Мета**: такі дослідження мають відповідати на конкретні наукові запитання і
  давати значущі інсайти. Підхід методичний, з чітко визначеними цілями.
  Бенчмарки мають бути добре структурованими та достатньо великими, щоб
  забезпечити статистично значущі результати.

Хоча можна перетворити одне з дослідницьких (exploratory) досліджень у
workhorse, я наполегливо
рекомендую починати збір даних з нуля. Зробіть так, щоб застарілі або помилкові
дані було максимально складно «протягнути» у ваш бенчмарковий сетап.

Дослідницькі (exploratory) дослідження вже повинні дати розумну оцінку
потрібного часу для
бенчмарків. Завжди закладайте достатньо часу на можливі збої та передбачайте
можливість відновлення виконання, якщо, наприклад, колега випадково зупинить
ваше завдання. Слідкуйте за результатами під час роботи бенчмарків — не хочете
чекати тиждень, а потім виявити, що забули зберегти розв’язки.

Особисто я структурую workhorse дослідження так:

1. **Гіпотеза або дослідницьке питання** Чітко сформулюйте гіпотезу або
   дослідницьке питання, яке виникло під час дослідницької (exploratory) фази.

2. **Дизайн експерименту** Розробіть детальний план експерименту, включно з
   набором екземплярів, моделями/конфігураціями для оцінювання та метриками,
   які потрібно зібрати.

3. **Бенчмарковий сетап** Реалізуйте надійний бенчмарковий фреймворк, що
   підтримує відтворюваність та ефективне виконання.

4. **Збір даних** Запустіть експерименти, забезпечивши збір і збереження всіх
   релевантних даних у структурованому та надійному форматі.

5. **Аналіз даних** Проаналізуйте результати, використовуючи відповідні
   статистичні та візуалізаційні методи.

6. **Обговорення висновків** Інтерпретуйте результати й обговоріть їхні
   наслідки в контексті початкової гіпотези або дослідницького питання.

7. **Загрози валідності** Розгляньте потенційні загрози валідності ваших
   висновків, такі як упередження у виборі екземплярів, припущення моделі або
   процедури оцінювання.

## Вибір бенчмарк-набору екземплярів

Побудова бенчмарк-набору екземплярів часто складніша, ніж здається на перший
погляд, особливо коли немає усталених бенчмарків. Навіть якщо такі набори є,
вони можуть бути невдало підібрані за розміром або менш реалістичні, ніж
очікувалося. Ба більше, деякі на вигляд «реалістичні» набори могли мати частину
оригінальних даних заміненою рівномірно випадковими значеннями для збереження
конфіденційності — і часто без усвідомлення, що такі зміни суттєво впливають на
характеристики задачі. Створення якісного бенчмарк-набору — це певною мірою
мистецтво. Помітний приклад — колекція
[MIPLIB](https://link.springer.com/article/10.1007/s12532-020-00194-3), яка сама
по собі є науковим внеском.

Якщо у вас вже є розгорнуте рішення, процес значно простіший. Ви можете зібрати
екземпляри, які розв’язувалися раніше, і використати їх (або репрезентативну
підмножину) як бенчмарк. Доцільно провести базовий аналіз даних, щоб подивитися
на розподіл характеристик екземплярів; наприклад, може виявитися, що 99%
екземплярів тривіальні, а решта 1% значно складніші й важливіші для покращення.
У більшості випадків базових доменних знань та здорового глузду достатньо, щоб
скласти корисний бенчмарк-набір без надмірно креативних рішень.

Якщо ж ви не в такому сприятливому становищі, перший крок — перевірити, чи є
публічні дані для вашої задачі або достатньо схожої. Наприклад, хоча широко
використовуваний бенчмарк
[TSPLIB](http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/) для задачі
комівояжера (TSP) містить лише інформацію про відстані, з нього відносно легко
згенерувати екземпляри задачі маршрутизації транспортних засобів із місткістю (CVRP), що дозволяє
повторно використати добре структуровані й складні входи для спорідненої задачі.
Це можна зробити, випадково обравши депо і призначивши місткість транспорту на
основі частки евристичного розв’язку TSP. Якщо ви берете готові екземпляри,
переконайтеся, що вони лишаються складними; вони можуть бути з іншої епохи або
погано спроєктовані, адже не все, що є онлайн, високої якості (хоча, сподіваюся,
цей праймер — так).

Якщо відповідних публічних бенчмарків немає, доведеться генерувати власні
екземпляри. Навіть за наявності публічних бенчмарків створення своїх може бути
корисним: ви зможете генерувати додаткові екземпляри, контролювати окремі
параметри та систематично оцінювати вплив зміни одного чинника на продуктивність
моделі. У типових наборах різноманітні екземпляри можуть «змішувати» впливи
параметрів, ускладнюючи їх ізоляцію без великих даних і акуратного
статистичного дизайну. Водночас важливо зберігати різноманітність у загальному
наборі екземплярів, щоб модель залишалася стійкою і здатною працювати в широкому
діапазоні сценаріїв.

> :video:
>
> Щоб глибше зрозуміти різноманітність бенчмарк-екземплярів, зверніть увагу на
> концепцію **Instance Space Analysis**. Кейт Сміт-Майлз має змістовну
> [30-хвилинну доповідь на цю тему](https://www.youtube.com/watch?v=-2t2c9-snf0),
> де розповідає, як аналіз простору екземплярів допомагає краще добирати та
> генерувати екземпляри.

Коли ви реалізуєте власну генерацію екземплярів, часто можна скористатися
готовими інструментами. Наприклад,
[NetworkX має вичерпний набір генераторів випадкових графів](https://networkx.org/documentation/stable/reference/generators.html),
які можна адаптувати під різні постановки задач. Зазвичай потрібно провести
exploratory дослідження, щоб визначити, який генератор найкраще відповідає
вимогам конкретної задачі. Для генерації інших типів значень можна експериментувати
з різними випадковими розподілами. Один особливо ефективний прийом — використання
зображень для визначення просторових або вартісних розподілів, наприклад,
вважаючи інтенсивності пікселів імовірностями вибірки.

> [!TIP]
>
> Також варто не об’єднувати всі екземпляри в один набір, а оцінювати
> продуктивність окремо по різних бенчмарк-групах. Такий підхід часто виявляє
> цікаві й змістовні відмінності в продуктивності.

Окремо варто наголосити на важливості генерування та збереження повноцінних
файлів екземплярів, а не лише покладання на seed псевдовипадкового генератора.
Це повторювана проблема, з якою я стикався і серед досвідчених колег, і серед
студентів. Хоча псевдовипадкові генератори корисні для введення випадковості,
що відтворюється, вони не є заміною постійно збережених даних. (До речі, я бачив
надто багато випадків, коли студент несвідомо обчислював середнє по кількох
запусках з одним і тим самим seed.) Теоретично seed у поєднанні з кодом має
достатньо відтворити експеримент, але на практиці код «старіє» швидше за дані.
Особливо це стосується C++: відтворюваність може бути гіршою, ніж очікується,
через тонкі прояви undefined behavior навіть у досвідчених програмістів.

## Ефективне управління бенчмарками

Управління бенчмарковими даними швидко ускладнюється, особливо коли ви ведете
кілька експериментів і дослідницьких питань паралельно. Наступні стратегії
допоможуть зберігати порядок і надійність результатів:

- **Структура папок:** підтримуйте чітку й послідовну ієрархію папок для
  експериментів. Типовий сетап — це верхньорівнева директорія `evaluations` з
  описовими підпапками для кожного експерименту. Наприклад:

  ```
  evaluations
  ├── tsp
  │   ├── 2023-11-18_random_euclidean
  │   │   ├── PRIVATE_DATA
  │   │   │   ├── ... all data for debugging and internal use
  │   │   ├── PUBLIC_DATA
  │   │   │   ├── ... curated data intended for sharing
  │   │   ├── _utils               # optional
  │   │   │   ├── ... shared utility functions to keep top level clean
  │   │   ├── README.md            # Brief description of the experiment
  │   │   ├── 00_generate_instances.py
  │   │   ├── 01_run_experiments.py
  │   │   ├── ...
  │   ├── 2023-11-18_tsplib
  │   │   ├── PRIVATE_DATA
  │   │   │   ├── ... debugging data
  │   │   ├── PUBLIC_DATA
  │   │   │   ├── ... selected shareable data
  │   │   ├── README.md
  │   │   ├── 01_run_experiments.py
  │   │   ├── ...
  ```

- **Документація:** легко забути, чому або коли проводився конкретний
  експеримент. Завжди додавайте короткий `README.md` з ключовими нотатками.
  Документ не має бути ідеально відшліфованим на старті, але має фіксувати
  основний контекст. Чим важливіший експеримент, тим корисніше повернутися до
  документації та доповнити її, коли експеримент уже триває і ви встигаєте
  осмислити мету та результати.

- **Надмірність:** надмірна тривога щодо дублювання даних і коду зазвичай
  зайва. Оцінювальні сетапи — не продакшн-системи, і їх не очікується підтримувати
  довгостроково. Ба більше, надмірність, особливо в утилітних функціях, може
  спростити рефакторинг. Старі експерименти можуть залишатися на старих версіях
  коду, а оновлення — застосовуватися вибірково. Рекомендується додавати короткий
  changelog у кожен утилітний файл, щоб позначати версію. Це можна розглядати як
  легковагове керування залежностями. Хоч копіювання-вставлення може здаватися
  «неправильним» інженеру ПЗ, цей шматок роботи зазвичай статичний заради
  відтворюваності, а не для активного супроводу.

- **Розгорнуті приватні та прості публічні дані:** організуйте дані в дві
  секції: одну для приватного використання та одну для публічного поширення.
  Приватна секція має містити всі сирі та проміжні дані, щоб можна було
  розслідувати аномалії чи несподівану поведінку. Публічна секція має бути
  стислою, відібраною і оптимізованою для аналізу та поширення. Якщо приватних
  даних стає надто багато, варто перенести їх у зовнішнє сховище і залишити
  посилання/нотатку про місце зберігання — бажано з надією, що це більше не
  знадобиться. Якщо експерименти невеликі, можливо, ви зможете зберігати всі
  дані в публічній секції.

- **Гнучкість експериментів:** проєктуйте експерименти так, щоб їх можна було
  зупиняти й продовжувати, а також додавати нові моделі чи конфігурації без
  перезапуску всього процесу. Така гнучкість особливо цінна в exploratory
  дослідженнях з частими ітераціями, а також у великих, довготривалих запусках.
  Чим довше триває експеримент, тим більша ймовірність переривань через оновлення
  системи, збої мережі або інші непередбачувані події.

- **Паралелізація:** швидке отримання результатів допомагає зберігати темп і
  фокус. Навчіться використовувати кластер або хмарну інфраструктуру для
  паралельного запуску експериментів. Хоча є початковий поріг входу, зусилля на
  впровадження паралелізації зазвичай невеликі порівняно з виграшем у швидкості.

> [!TIP]
>
> Оскільки наявні інструменти не повністю відповідали моїм вимогам, я створив
> [AlgBench](https://github.com/d-krupke/AlgBench) для керування результатами
> бенчмаркінгу та [Slurminade](https://github.com/d-krupke/slurminade), щоб
> спростити розподіл експериментів на кластерах через інтерфейс на основі
> декораторів. Втім, нині можуть існувати ефективніші рішення, особливо зі
> спільноти машинного навчання. Якщо знаєте корисні інструменти, буду радий
> почути про них і з задоволенням розгляну їхній потенціал.
